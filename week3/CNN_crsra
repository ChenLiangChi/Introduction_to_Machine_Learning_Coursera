import tensorflow as tf

@tf.function
# Fully connection layer with ReLU activation layer
def fully_connect(x):
    return tf.nn.relu(tf.matmul(x, W) + b)
# CNN layer 1 with ReLU activation function
# x_cnn diemension: [None, 28, 28, 1]--[Batch, Height, Width, channel_in]
# Note: channel_in represents the channels of input, i.e. RGB image -> 3 channels
# Stride: [1, stride, stride, 1]
def cnn1(x_cnn):
    conv1_preact = tf.nn.conv2d(x_cnn, W1_cnn, strides=[1, 1, 1, 1], padding='SAME') + b1_cnn
    return tf.nn.relu(conv1_preact)
# CNN layer 2 with ReLU activation function 
def cnn2(conv1):
    conv2_preact = tf.nn.conv2d(conv1, W2_cnn, strides=[1, 1, 1, 1], padding='SAME') + b2_cnn
    return tf.nn.relu(conv2_preact)

W = tf.Variable(tf.random.truncated_normal([784, 10], stddev=1))
b = tf.Variable(tf.zeros([10]))

# Create convolutional kernel variable of layer 1
# W1_cnn diemension: [5, 5, 1, 16]--[Height, Width, channel_in, channel_out]
# Note: channel_out represents the numbers of filters
W1_cnn = tf.Variable(tf.random.truncated_normal([5, 5, 1, 16], stddev=0.1))
b1_cnn = tf.Variable(tf.zeros([16]))

# Create convolutional kernel variable of layer 2
W2_cnn = tf.Variable(tf.random.truncated_normal([5, 5, 16, 32], stddev=0.1))
b2_cnn = tf.Variable(tf.zeros([32]))

# Create example input data
x = tf.Variable(tf.zeros([1, 28, 28, 1], dtype=tf.float32))

# Apply input to CNN 
conv1 = cnn1(x)
conv2 = cnn2(conv1)

# Print the shape of the result of each layers
tf.print("Original shape of input data x: ", x.shape)
tf.print("Shape of conv1: ", conv1.shape)
tf.print("Shape of conv2 feature maps before pooling: ", conv2.shape)

# Pool the output of CNN layer 2
max_pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
avg_pool2 = tf.nn.avg_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
tf.print("Shape of conv2 feature maps after max pooling: ", max_pool2.shape)
tf.print("Shape of conv2 feature maps after avg pooling: ", avg_pool2.shape)